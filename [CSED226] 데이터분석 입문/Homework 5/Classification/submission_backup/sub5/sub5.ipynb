{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "You can use any models or methods you have learned in class (Decision Tree, Ensembles, SVM, etc.) *except kNN   \n",
    "In the classification task, we will use Weighted F1 score for the evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Versions\n",
    "> 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]   \n",
    "> numpy: 2.1.0   \n",
    "> pandas: 2.2.2   \n",
    "> matplotlib.pyplot: 3.9.2   \n",
    "> sklearn: 1.5.2   \n",
    "> scipy: 1.14.1   \n",
    "> seaborn: 0.13.2   \n",
    "> xgboost: 2.1.2   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from importlib import reload\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'settings' from 'd:\\\\Workspace\\\\Dataanalysis_homework_5\\\\Classification\\\\settings.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from settings import *\n",
    "reload(sys.modules['settings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG:\n",
    "    log_file = open(LOG_FILE, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GP', 'GS', 'MIN']\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv(DATA_PATH + \"\\\\train.csv\")\n",
    "testing_data = pd.read_csv(DATA_PATH + \"\\\\test.csv\")\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "training_data[\"position\"] = label_encoder.fit_transform(training_data[\"position\"])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "drop_col = []\n",
    "for col in training_data.columns:\n",
    "    if col not in testing_data.columns:\n",
    "        drop_col.append(col)\n",
    "drop_col.remove(\"position\")\n",
    "training_data.drop(columns=drop_col, inplace=True)\n",
    "training_data.drop(columns=[\"SEASON_ID\", \"TEAM_ID\"], inplace=True)\n",
    "testing_data.drop(columns=[\"ID\", \"SEASON_ID\", \"TEAM_ID\"], inplace=True)\n",
    "\n",
    "# Process missing values\n",
    "if METHOD_PROCESSING_MISSING_VALUES == \"drop\":\n",
    "    training_data.dropna(inplace=True)\n",
    "elif METHOD_PROCESSING_MISSING_VALUES == \"mean\":\n",
    "    training_data.fillna(training_data.mean(), inplace=True)\n",
    "elif METHOD_PROCESSING_MISSING_VALUES == \"med\":\n",
    "    training_data.fillna(training_data.median(), inplace=True)\n",
    "elif METHOD_PROCESSING_MISSING_VALUES == \"mode\":\n",
    "    training_data.fillna(training_data.mode(), inplace=True)\n",
    "else:\n",
    "    raise(\"Invalid method for processing missing values\")\n",
    "\n",
    "# Divide data into X and y\n",
    "X = training_data.drop(columns=[\"position\"])\n",
    "y = training_data[\"position\"]\n",
    "\n",
    "# Feature scaling\n",
    "if SCALER == None:\n",
    "    pass\n",
    "elif SCALER == \"std\":\n",
    "    scaler = StandardScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    testing_data = pd.DataFrame(scaler.transform(testing_data), columns=testing_data.columns)\n",
    "elif SCALER == \"minmax\":\n",
    "    scaler = MinMaxScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    testing_data = pd.DataFrame(scaler.transform(testing_data), columns=testing_data.columns)\n",
    "else:\n",
    "    raise(\"Invalid scaler\")\n",
    "\n",
    "# TSNE or PCA\n",
    "if ADD_TSNE[0] == True:\n",
    "    tsne = TSNE(n_components=ADD_TSNE[1])\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    X = pd.concat([X, pd.DataFrame(X_tsne)], axis=1)\n",
    "if ADD_PCA[0] == True:\n",
    "    pca = PCA(n_components=ADD_PCA[1])\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    X = pd.concat([X, pd.DataFrame(X_pca)], axis=1)\n",
    "\n",
    "# Info. of data\n",
    "print(drop_col)\n",
    "print(len(X.columns))\n",
    "training_data\n",
    "\n",
    "# logging\n",
    "if LOG:\n",
    "    try: \n",
    "        log_file.write(\"Number of columns: \" + str(len(X.columns)) + \"\\n\")\n",
    "        log_file.write(\"Number of rows: \" + str(len(X)) + \"\\n\")\n",
    "        log_file.write(\"Columns: \")\n",
    "        for col in X.columns:\n",
    "            log_file.write(col + \" \")\n",
    "        log_file.write(\"\\n\")\n",
    "    except IOError:\n",
    "        print(\"logger is not working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: PLAYER_AGE, FGA, FG_PCT, FG3A, FT_PCT, OREB, DREB, REB, AST, STL, BLK, TOV, PF, PTS, \n"
     ]
    }
   ],
   "source": [
    "# Remove unrelated feature using RFE.\n",
    "if 0 > NUM_SELECTED_FEATURES or NUM_SELECTED_FEATURES > len(X.columns):\n",
    "    raise(\"Invalid number of selected features\")\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=NUM_SELECTED_FEATURES)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "print(\"Selected features: \", end=\"\")\n",
    "for i in range(X.shape[1]):\n",
    "    if rfe.support_[i]:\n",
    "        print(X.columns[i], end=\", \")\n",
    "print()\n",
    "\n",
    "X_selected = rfe.transform(X)\n",
    "testing_data_selected = rfe.transform(testing_data)\n",
    "\n",
    "# Logging\n",
    "if LOG:\n",
    "    try:\n",
    "        log_file.write(\"Number of selected features: \" + str(NUM_SELECTED_FEATURES) + \"\\n\")\n",
    "        log_file.write(\"Selected features: \")\n",
    "        for i in range(X.shape[1]):\n",
    "            if rfe.support_[i]:\n",
    "                log_file.write(X.columns[i] + \", \")\n",
    "        log_file.write(\"\\n\")\n",
    "    except IOError:\n",
    "        print(\"logger is not working\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if METHOD_MODEL == \"xgb\":\n",
    "    model = XGBClassifier()\n",
    "\n",
    "    if LOG:\n",
    "        try:\n",
    "            log_file.write(\"Model: \" + str(type(model)) + \"\\n\")\n",
    "        except IOError:\n",
    "            print(\"logger is not working\")\n",
    "\n",
    "    if METHOD_VALIDATION == \"kf\":\n",
    "        grid = GridSearchCV(estimator=model, param_grid=XGBOOST_PARAMS, cv=kf, n_jobs=-1, verbose=1)\n",
    "        grid.fit(X_selected, y)\n",
    "        print(grid.best_params_)\n",
    "        print(grid.best_score_)\n",
    "        if LOG:\n",
    "            try:\n",
    "                log_file.write(\"Best parameters: \" + str(grid.best_params_) + \"\\n\")\n",
    "                log_file.write(\"Best score: \" + str(grid.best_score_) + \"\\n\")\n",
    "            except IOError:\n",
    "                print(\"logger is not working\")\n",
    "\n",
    "\n",
    "    elif METHOD_VALIDATION == \"skf\":\n",
    "        grid = GridSearchCV(estimator=model, param_grid=XGBOOST_PARAMS, cv=skf, n_jobs=-1, verbose=1)\n",
    "        grid.fit(X_selected, y)\n",
    "        print(grid.best_params_)\n",
    "        print(grid.best_score_)\n",
    "        if LOG:\n",
    "            try:\n",
    "                log_file.write(\"Best parameters: \" + str(grid.best_params_) + \"\\n\")\n",
    "                log_file.write(\"Best score: \" + str(grid.best_score_) + \"\\n\")\n",
    "            except IOError:\n",
    "                print(\"logger is not working\")\n",
    "\n",
    "    elif METHOD_VALIDATION == None:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        print(f1_score(y_val, y_pred, average=\"weighted\"))\n",
    "        print(classification_report(y_val, y_pred))\n",
    "        if LOG:\n",
    "            try:\n",
    "                log_file.write(\"F1 score: \" + str(f1_score(y_val, y_pred, average=\"weighted\")) + \"\\n\")\n",
    "                log_file.write(\"Classification report: \\n\" + classification_report(y_val, y_pred) + \"\\n\")\n",
    "            except IOError:\n",
    "                print(\"logger is not working\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise(\"Invalid method for validation\")\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.6500997289346906\n"
     ]
    }
   ],
   "source": [
    "if METHOD_MODEL == \"svc\":\n",
    "    model = SVC()\n",
    "\n",
    "    if LOG:\n",
    "        try:\n",
    "            log_file.write(\"Model: \" + str(type(model)) + \"\\n\")\n",
    "        except IOError:\n",
    "            print(\"logger is not working\")\n",
    "\n",
    "    if METHOD_VALIDATION == \"kf\":\n",
    "        grid = GridSearchCV(estimator=model, param_grid=SVC_PARAMS, cv=kf, n_jobs=-1, verbose=1)\n",
    "        grid.fit(X_selected, y)\n",
    "        print(grid.best_params_)\n",
    "        print(grid.best_score_)\n",
    "        if LOG:\n",
    "            try:\n",
    "                log_file.write(\"Best parameters: \" + str(grid.best_params_) + \"\\n\")\n",
    "                log_file.write(\"Best score: \" + str(grid.best_score_) + \"\\n\")\n",
    "            except IOError:\n",
    "                print(\"logger is not working\")\n",
    "\n",
    "\n",
    "    elif METHOD_VALIDATION == \"skf\":\n",
    "        grid = GridSearchCV(estimator=model, param_grid=SVC_PARAMS, cv=skf, n_jobs=-1, verbose=1)\n",
    "        grid.fit(X_selected, y)\n",
    "        print(grid.best_params_)\n",
    "        print(grid.best_score_)\n",
    "        if LOG:\n",
    "            try:\n",
    "                log_file.write(\"Best parameters: \" + str(grid.best_params_) + \"\\n\")\n",
    "                log_file.write(\"Best score: \" + str(grid.best_score_) + \"\\n\")\n",
    "            except IOError:\n",
    "                print(\"logger is not working\")\n",
    "\n",
    "    elif METHOD_VALIDATION == None:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        print(f1_score(y_val, y_pred, average=\"weighted\"))\n",
    "        print(classification_report(y_val, y_pred))\n",
    "        if LOG:\n",
    "            try:\n",
    "                log_file.write(\"F1 score: \" + str(f1_score(y_val, y_pred, average=\"weighted\")) + \"\\n\")\n",
    "                log_file.write(\"Classification report: \\n\" + classification_report(y_val, y_pred) + \"\\n\")\n",
    "            except IOError:\n",
    "                print(\"logger is not working\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise(\"Invalid method for validation\")\n",
    "\n",
    "    best_model = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final training for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID        position\n",
      "0        1           Guard\n",
      "1        2           Guard\n",
      "2        3           Guard\n",
      "3        4          Center\n",
      "4        5          Center\n",
      "...    ...             ...\n",
      "1995  1996         Forward\n",
      "1996  1997           Guard\n",
      "1997  1998         Forward\n",
      "1998  1999  Forward-Center\n",
      "1999  2000           Guard\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "model = best_model\n",
    "y_pred = model.predict(testing_data_selected)\n",
    "submission_df = pd.DataFrame()\n",
    "temp = pd.read_csv(DATA_PATH + \"\\\\test.csv\")\n",
    "submission_df[\"ID\"] = temp[\"ID\"]\n",
    "submission_df[\"position\"] = label_encoder.inverse_transform(y_pred)\n",
    "print(submission_df)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Close log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log_file.close()\n",
    "except IOError:\n",
    "    print(\"logger is not working\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
